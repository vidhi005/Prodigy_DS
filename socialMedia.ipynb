{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_zip(zip_file_path, output_folder):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_folder)\n",
    "zip_file_path = '/content/Twitter.zip'\n",
    "output_folder = '/content/Twitter'\n",
    "extract_zip(zip_file_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=['Tweet_ID','Entity','Sentiment','Tweet_content']\n",
    "\n",
    "train = pd.read_csv(\"/content/Twitter/twitter_training.csv\",\n",
    "                    sep=',',names=column_names)\n",
    "\n",
    "\n",
    "validation = pd.read_csv(\"/content/Twitter/twitter_validation.csv\",\n",
    "                    sep=',',names=column_names)\n",
    "print(train.shape)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation.shape)\n",
    "validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\tTrain dataset\")\n",
    "print(train.isna().sum())\n",
    "print(\"*\"*40)\n",
    "print(\"\\t\\t\\tValidation dataset\")\n",
    "print(validation.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\tTrain dataset\")\n",
    "print(train.duplicated().sum())\n",
    "print(\"*\"*40)\n",
    "print(\"\\t\\t\\tValidation dataset\")\n",
    "print(validation.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "def remove_urls(text):\n",
    "    \"\"\"Berilgan matndan URL larini o'chiradi\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Berilgan matndan emojilarni o'chiradi\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emojilar\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # simvollar va diagrammalar\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport va turli joylar\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # davlat bayroqlari\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # alamatchilik belgilari\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Tweet_content'] = train['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "train['Tweet_content'] = train['Tweet_content'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "validation['Tweet_content'] = validation['Tweet_content'].apply(lambda x: remove_emojis(x))\n",
    "validation['Tweet_content'] = validation['Tweet_content'].apply(lambda x: remove_urls(x))\n",
    "train['text_lens']=train['Tweet_content'].apply(lambda x: len(x))\n",
    "validation['text_lens']=validation['Tweet_content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].pie(train['Sentiment'].value_counts(),labels=train['Sentiment'].value_counts().index,autopct='%.f%%')\n",
    "ax[1].pie(validation['Sentiment'].value_counts(),labels=validation['Sentiment'].value_counts().index,autopct='%.f%%')\n",
    "\n",
    "fig.suptitle(\"Proportions of target classes\")\n",
    "ax[0].set_title(\"Train dataset\")\n",
    "ax[1].set_title(\"Validation dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ,ax = plt.subplots(2,1,figsize=(14,16))\n",
    "\n",
    "sns.kdeplot(data=train,x='text_lens',hue='Sentiment',ax=ax[0])\n",
    "sns.kdeplot(data=validation,x='text_lens',hue='Sentiment',ax=ax[1])\n",
    "\n",
    "fig.suptitle(\"Length of tweets in datasets\")\n",
    "ax[0].set_title(\"Train dataset\")\n",
    "ax[1].set_title(\"Validation dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=train.groupby(by=[\"Entity\",\"Sentiment\"]).count().reset_index()\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(data=data1,x=\"Entity\",y=\"Tweet_ID\",hue='Sentiment')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Brand\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.grid()\n",
    "plt.title(\"Distribution of tweets per Branch and Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_text = ''.join(train[train[\"Sentiment\"]==\"Positive\"].Tweet_content)\n",
    "wordcloud = WordCloud(\n",
    "    max_font_size=100,\n",
    "    max_words=100,\n",
    "    background_color=\"black\",\n",
    "    scale=10,\n",
    "    width=800,\n",
    "    height=800\n",
    ").generate(word_cloud_text)\n",
    "#Figure properties\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_text_negative = ''.join(train[train[\"Sentiment\"]==\"Negative\"].Tweet_content)\n",
    "wordcloud_negative = WordCloud(\n",
    "    max_font_size=100,\n",
    "    max_words=100,\n",
    "    background_color=\"black\",\n",
    "    scale=10,\n",
    "    width=800,\n",
    "    height=800\n",
    ").generate(word_cloud_text_negative)\n",
    "#Figure properties\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_text = ''.join(train[train[\"Sentiment\"]==\"Neutral\"].Tweet_content)\n",
    "wordcloud = WordCloud(\n",
    "    max_font_size=100,\n",
    "    max_words=100,\n",
    "    background_color=\"black\",\n",
    "    scale=10,\n",
    "    width=800,\n",
    "    height=800\n",
    ").generate(word_cloud_text)\n",
    "#Figure properties\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,2,figsize=(16,6))\n",
    "\n",
    "sns.boxplot(data=train,x='text_lens',ax=ax[0])\n",
    "sns.boxplot(data=validation,x='text_lens',ax=ax[1])\n",
    "\n",
    "ax[0].set_title(\"Train dataset\")\n",
    "ax[1].set_title(\"Validation dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "#remove outliers\n",
    "train = remove_outlier(train,'text_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens=[]\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "train['preprocessed_text']= train['Tweet_content'].apply(lambda x: preprocess(x))\n",
    "validation['preprocessed_text']= validation['Tweet_content'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train ,X_test , y_train, y_test = train_test_split(\n",
    "    train[['preprocessed_text']],\n",
    "    train[['Sentiment']],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "\n",
    "X_train_vect= vectorizer.fit_transform(X_train['preprocessed_text'])\n",
    "X_test_vect= vectorizer.transform(X_test['preprocessed_text'])\n",
    "\n",
    "y_train=y_train['Sentiment'].map({\"Positive\":0,\"Negative\":1,\"Neutral\":2,\"Irrelevant\":3})\n",
    "y_test=y_test['Sentiment'].map({\"Positive\":0,\"Negative\":1,\"Neutral\":2,\"Irrelevant\":3})\n",
    "validation_X = vectorizer.transform(validation['preprocessed_text'])\n",
    "validation_y = validation['Sentiment'].map({\"Positive\":0,\"Negative\":1,\"Neutral\":2,\"Irrelevant\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_vect,y_train)\n",
    "y_predict= model.predict(X_test_vect)\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vect,y_train)\n",
    "y_predict= model.predict(X_test_vect)\n",
    "\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_predict= model_extra.predict(validation_X)\n",
    "print(classification_report(validation_y,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_extra.pkl', 'wb') as file:\n",
    "    pickle.dump(model_extra, file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    " \n",
    "train.to_csv(\"train_data.csv\")\n",
    "validation.to_csv('validation_data.csv')\n",
    "text = 'Your support team is useless'\n",
    "text1='Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it/RMTrgF '\n",
    "text_final = vectorizer.transform([text1])\n",
    "\n",
    "predict = model_extra.predict(text_final)\n",
    "list(predict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kayfiyatlar={0:\"Positive\",1:\"Negative\",2:\"Neutral\",3:\"Irrelevant\"}\n",
    "kayfiyatlar[list(predict)[0]]\n",
    "train[train['Sentiment']=='Neutral']['Tweet_content'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Concatenate, Dense, Input\n",
    "from keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/Twitter/twitter_training.csv',\n",
    "                 usecols=[0,1,2,3], names=['Tweet_ID','entity', 'sentiment', 'tweet_content'])\n",
    "\n",
    "df = df[['Tweet_ID', 'entity', 'tweet_content', 'sentiment']]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.duplicated().sum()\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let’s\": 'let us',\n",
    "                r\"'s\":  ' is',\n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan’t\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',\n",
    "                ',': ' ,',\n",
    "                '.': ' .',\n",
    "                '!': ' !',\n",
    "                '?': ' ?',\n",
    "                '\\s+': ' '}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "x = df['tweet_content'].apply(lambda p: clean_text(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "REPLACE_WITH_SPACE = re.compile(\"(@)\")\n",
    "SPACE = \" \"\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "english_stop_words = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#1\n",
    "def reviews(reviews):\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line.lower()) for line in reviews]\n",
    "\n",
    "    return reviews\n",
    "#2\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split()  if word not in english_stop_words]))\n",
    "    return removed_stop_words\n",
    "#3\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "#1\n",
    "reviewtweet = reviews(x)\n",
    "#2\n",
    "no_stop_words_tweet = remove_stop_words(reviewtweet)\n",
    "#3\n",
    "stemmed_reviews_tweet = get_stemmed_text(no_stop_words_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "\n",
    "max_words = 8000\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words = max_words,\n",
    "    filters = '\"#$%&()*+-/:;<=>@[\\]^_{|}~'\n",
    ")\n",
    "tokenizer.fit_on_texts(stemmed_reviews_tweet)\n",
    "\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(y)\n",
    "\n",
    "x = tokenizer.texts_to_sequences(stemmed_reviews_tweet)\n",
    "x = pad_sequences(x, maxlen = 300)\n",
    "\n",
    "y = np.array(label_tokenizer.texts_to_sequences(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn = Sequential()\n",
    "model_simple_rnn.add(Embedding(input_dim = max_words, output_dim = 128, input_length = 300))\n",
    "model_simple_rnn.add(SpatialDropout1D(0.2))\n",
    "model_simple_rnn.add(SimpleRNN(128, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model_simple_rnn.add(Dense(128, activation = 'relu'))\n",
    "model_simple_rnn.add(Dropout(0.2))\n",
    "model_simple_rnn.add(Dense(5, activation = 'softmax'))\n",
    "model_simple_rnn.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "plot_model(model_simple_rnn, to_file=\"model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_simple_rnn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test,y_test) ,\n",
    "    epochs = 10,\n",
    "    batch_size = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Tweet_content', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = train[['Tweet_content', 'Sentiment']]\n",
    "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
    "data['Sentiment'] = data['Sentiment_label'].cat.codes\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "# Extract the training and testing texts and labels\n",
    "train_texts = data_train['Tweet_content'].tolist()\n",
    "train_labels = data_train['Sentiment'].tolist()\n",
    "test_texts = data_test['Tweet_content'].tolist()\n",
    "test_labels = data_test['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "num_labels = len(data['Sentiment_label'].cat.categories)\n",
    "train_labels_encoded = tf.one_hot(train_labels, num_labels)\n",
    "test_labels_encoded = tf.one_hot(test_labels, num_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n",
    "\n",
    "# Define the model architecture\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile and train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "batch_size = 16\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(train_dataset.batch(batch_size), epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Make predictions on the training and test datasets\n",
    "train_predictions = model.predict(train_dataset.batch(64))\n",
    "train_predicted_labels = np.argmax(train_predictions, axis=1)\n",
    "test_predictions = model.predict(test_dataset.batch(64))\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "train_predicted_sentiments = data['Sentiment_label'].cat.categories[train_predicted_labels]\n",
    "test_predicted_sentiments = data['Sentiment_label'].cat.categories[test_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "train_true_labels = data_train['Sentiment_label']\n",
    "test_true_labels = data_test['Sentiment_label']\n",
    "\n",
    "# Calculate the classification report for the training set\n",
    "train_classification_rep = classification_report(train_true_labels, train_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", train_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "train_confusion_mat = confusion_matrix(train_true_labels, train_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(train_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "train_display = ConfusionMatrixDisplay(confusion_matrix=train_confusion_mat, display_labels=labels)\n",
    "train_display.plot(cmap='Blues')\n",
    "plt.title(\"Training Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the classification report for the test set\n",
    "test_classification_rep = classification_report(test_true_labels, test_predicted_sentiments)\n",
    "print(\"Test Set - Classification Report:\\n\", test_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_confusion_mat = confusion_matrix(test_true_labels, test_predicted_sentiments)\n",
    "\n",
    "# Plot the confusion matrix for the test set\n",
    "test_display = ConfusionMatrixDisplay(confusion_matrix=test_confusion_mat, display_labels=labels)\n",
    "test_display.plot(cmap='Blues')\n",
    "plt.title(\"Test Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = validation[['Tweet_content', 'Sentiment']]\n",
    "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
    "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
    "\n",
    "# Extract the training and testing texts and labels\n",
    "val_texts = data_val['Tweet_content'].tolist()\n",
    "val_labels = data_val['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
    "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
    "\n",
    "# Make predictions on the training and test datasets\n",
    "\n",
    "val_predictions = model.predict(val_dataset.batch(64))\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "val_predicted_sentiments = data_val['Sentiment_label'].cat.categories[val_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "val_true_labels = data_val['Sentiment_label']\n",
    "\n",
    "# Calculate the classification report for the training set\n",
    "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", val_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(val_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
    "val_display.plot(cmap='Blues')\n",
    "plt.title(\"Validation Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel, RobertaTokenizer\n",
    "\n",
    "#Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "#Load and preprocess the data\n",
    "data = train[['Tweet_content', 'Sentiment']]\n",
    "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
    "data['Sentiment'] = data['Sentiment_label'].cat.codes\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "#Extract the training and testing texts and labels\n",
    "train_texts = data_train['Tweet_content'].tolist()\n",
    "train_labels = data_train['Sentiment'].tolist()\n",
    "test_texts = data_test['Tweet_content'].tolist()\n",
    "test_labels = data_test['Sentiment'].tolist()\n",
    "\n",
    "#Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "#Convert the labels to one-hot encoding\n",
    "num_labels = len(data['Sentiment_label'].cat.categories)\n",
    "train_labels_encoded = tf.one_hot(train_labels, num_labels)\n",
    "test_labels_encoded = tf.one_hot(test_labels, num_labels)\n",
    "\n",
    "#Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n",
    "\n",
    "#Define the model architecture\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :]) # Pooling the output\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "#Compile and train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "#Use smaller batch size\n",
    "batch_size = 16\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(train_dataset.batch(batch_size), epochs=2)\n",
    "\n",
    "#Evaluate the model\n",
    "model.evaluate(test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_dataset.batch(64))\n",
    "train_predicted_labels = np.argmax(train_predictions, axis=1)\n",
    "test_predictions = model.predict(test_dataset.batch(64))\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "train_predicted_sentiments = data['Sentiment_label'].cat.categories[train_predicted_labels]\n",
    "test_predicted_sentiments = data['Sentiment_label'].cat.categories[test_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "train_true_labels = data_train['Sentiment_label']\n",
    "test_true_labels = data_test['Sentiment_label']\n",
    "\n",
    "# Calculate the classification report for the training set\n",
    "train_classification_rep = classification_report(train_true_labels, train_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", train_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "train_confusion_mat = confusion_matrix(train_true_labels, train_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(train_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "train_display = ConfusionMatrixDisplay(confusion_matrix=train_confusion_mat, display_labels=labels)\n",
    "train_display.plot(cmap='Blues')\n",
    "plt.title(\"Training Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "test_classification_rep = classification_report(test_true_labels, test_predicted_sentiments)\n",
    "print(\"Test Set - Classification Report:\\n\", test_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_confusion_mat = confusion_matrix(test_true_labels, test_predicted_sentiments)\n",
    "\n",
    "# Plot the confusion matrix for the test set\n",
    "test_display = ConfusionMatrixDisplay(confusion_matrix=test_confusion_mat, display_labels=labels)\n",
    "test_display.plot(cmap='Blues')\n",
    "plt.title(\"Test Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = validation[['Tweet_content', 'Sentiment']]\n",
    "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
    "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
    "\n",
    "# Extract the training and testing texts and labels\n",
    "val_texts = data_val['Tweet_content'].tolist()\n",
    "val_labels = data_val['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
    "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
    "\n",
    "# Make predictions on the training and test datasets\n",
    "\n",
    "val_predictions = model.predict(val_dataset.batch(64))\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "val_predicted_sentiments = data_val['Sentiment_label'].cat.categories[val_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "val_true_labels = data_val['Sentiment_label']\n",
    "\n",
    "# Calculate the classification report for the training set\n",
    "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", val_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(val_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
    "val_display.plot(cmap='Blues')\n",
    "plt.title(\"Validation Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFXLNetModel, XLNetTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = train[['Tweet_content', 'Sentiment']]\n",
    "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
    "data['Sentiment'] = data['Sentiment_label'].cat.codes\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data_train, data_test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "# Extract the training and testing texts and labels\n",
    "train_texts = data_train['Tweet_content'].tolist()\n",
    "train_labels = data_train['Sentiment'].tolist()\n",
    "test_texts = data_test['Tweet_content'].tolist()\n",
    "test_labels = data_test['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "num_labels = len(data['Sentiment_label'].cat.categories)\n",
    "train_labels_encoded = tf.one_hot(train_labels, num_labels)\n",
    "test_labels_encoded = tf.one_hot(test_labels, num_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n",
    "\n",
    "# Define the model architecture\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile and train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Use smaller batch size\n",
    "batch_size = 16\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(train_dataset.batch(batch_size), epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(test_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_dataset.batch(64))\n",
    "train_predicted_labels = np.argmax(train_predictions, axis=1)\n",
    "test_predictions = model.predict(test_dataset.batch(64))\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "train_predicted_sentiments = data['Sentiment_label'].cat.categories[train_predicted_labels]\n",
    "test_predicted_sentiments = data['Sentiment_label'].cat.categories[test_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "train_true_labels = data_train['Sentiment_label']\n",
    "test_true_labels = data_test['Sentiment_label']\n",
    "\n",
    "# Calculate the classification report for the training set\n",
    "train_classification_rep = classification_report(train_true_labels, train_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", train_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "train_confusion_mat = confusion_matrix(train_true_labels, train_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(train_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "train_display = ConfusionMatrixDisplay(confusion_matrix=train_confusion_mat, display_labels=labels)\n",
    "train_display.plot(cmap='Blues')\n",
    "plt.title(\"Training Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "test_classification_rep = classification_report(test_true_labels, test_predicted_sentiments)\n",
    "print(\"Test Set - Classification Report:\\n\", test_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_confusion_mat = confusion_matrix(test_true_labels, test_predicted_sentiments)\n",
    "\n",
    "# Plot the confusion matrix for the test set\n",
    "test_display = ConfusionMatrixDisplay(confusion_matrix=test_confusion_mat, display_labels=labels)\n",
    "test_display.plot(cmap='Blues')\n",
    "plt.title(\"Test Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = validation[['Tweet_content', 'Sentiment']]\n",
    "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
    "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
    "\n",
    "# Extract the training and testing texts and labels\n",
    "val_texts = data_val['Tweet_content'].tolist()\n",
    "val_labels = data_val['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
    "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
    "\n",
    "# Make predictions on the training and test datasets\n",
    "\n",
    "val_predictions = model.predict(val_dataset.batch(64))\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Convert the predicted labels to their original sentiment categories\n",
    "val_predicted_sentiments = data_val['Sentiment_label'].cat.categories[val_predicted_labels]\n",
    "\n",
    "# Convert the true labels to their original sentiment categories\n",
    "val_true_labels = data_val['Sentiment_label']\n",
    "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
    "print(\"Training Set - Classification Report:\\n\", val_classification_rep)\n",
    "\n",
    "# Generate the confusion matrix for the training set\n",
    "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
    "\n",
    "# Get the unique labels/categories from the true labels\n",
    "labels = np.unique(val_true_labels)\n",
    "\n",
    "# Plot the confusion matrix for the training set\n",
    "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
    "val_display.plot(cmap='Blues')\n",
    "plt.title(\"Validation Set - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
